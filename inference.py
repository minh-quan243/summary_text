import torch
import pickle
import numpy as np
from model import Seq2SeqWithAttention  # import ki·∫øn tr√∫c model b·∫°n ƒë√£ ƒë·ªãnh nghƒ©a
import re

# ===========================
# üß© C·∫§U H√åNH
# ===========================
VOCAB_SIZE = 20000
EMBEDDING_DIM = 256
HIDDEN_DIM = 512
MAX_TEXT_LEN = 200
MAX_SUMM_LEN = 40
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ===========================
# üìÇ LOAD MODEL & TOKENIZER
# ===========================
print("üîÑ ƒêang load model & tokenizer...")

with open("tokenizer_x.pkl", "rb") as f:
    tokenizer_x = pickle.load(f)
with open("tokenizer_y.pkl", "rb") as f:
    tokenizer_y = pickle.load(f)

model = Seq2SeqWithAttention(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM)
checkpoint = torch.load("best_model.pth", map_location=DEVICE)
model.load_state_dict(checkpoint["model_state_dict"])
model.to(DEVICE)
model.eval()

print("‚úÖ Model ƒë√£ load th√†nh c√¥ng!")

# ===========================
# üßπ H√ÄM X·ª¨ L√ù TEXT (Pytorch version)
# ===========================
def clean_text(text: str) -> str:
    """L√†m s·∫°ch text c∆° b·∫£n"""
    text = text.lower()
    text = re.sub(r"[^a-zA-Z0-9√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≠√¨·ªâƒ©·ªã√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµƒë\s]", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

def pad_sequences_torch(sequences, maxlen, padding_value=0):
    """H√†m pad sequence gi·ªëng Keras nh∆∞ng thu·∫ßn PyTorch"""
    padded = torch.full((len(sequences), maxlen), padding_value, dtype=torch.long)
    for i, seq in enumerate(sequences):
        trunc = seq[:maxlen]
        padded[i, :len(trunc)] = torch.tensor(trunc, dtype=torch.long)
    return padded

def preprocess_text(text, tokenizer, max_len):
    text = clean_text(text)
    tokens = tokenizer.texts_to_sequences([text])[0]
    padded = pad_sequences_torch([tokens], max_len)
    return padded.to(DEVICE)

# ===========================
# üîÅ SINH T√ìM T·∫ÆT
# ===========================
def generate_summary(model, input_text, tokenizer_x, tokenizer_y, max_text_len, max_summ_len, device):
    model.eval()
    reverse_word_index = {v: k for k, v in tokenizer_y.word_index.items()}

    src = preprocess_text(input_text, tokenizer_x, max_text_len)

    with torch.no_grad():
        encoder_outputs, hidden, cell = model.encoder(src)
        decoder_input = torch.ones(1, 1, dtype=torch.long, device=device)  # <START>
        decoded_tokens = []

        for _ in range(max_summ_len):
            prediction, hidden, cell, _ = model.decoder(decoder_input, hidden, cell, encoder_outputs)
            token = prediction.argmax(1, keepdim=True)

            if token.item() in [0, 2]:  # padding ho·∫∑c end token
                break

            decoded_tokens.append(token.item())
            decoder_input = token

    summary = " ".join([reverse_word_index.get(idx, "") for idx in decoded_tokens])
    return summary.strip()

# ===========================
# üß™ TEST M·ªòT V√ç D·ª§
# ===========================
if __name__ == "__main__":
    input_text = """
    The study investigates the effect of vitamin D supplementation on bone health in elderly individuals.
    Participants were given daily doses for six months and monitored for bone density improvements.
    Results showed significant benefits in maintaining bone strength and reducing fracture risk.
    """

    summary = generate_summary(
        model, input_text, tokenizer_x, tokenizer_y,
        MAX_TEXT_LEN, MAX_SUMM_LEN, DEVICE
    )

    print("\nüßæ INPUT:")
    print(clean_text(input_text))
    print("\n‚ú® SUMMARY:")
    print(summary)

    # Cho ph√©p nh·∫≠p nhi·ªÅu vƒÉn b·∫£n t√πy √Ω
    while True:
        text = input("\nNh·∫≠p ƒëo·∫°n vƒÉn c·∫ßn t√≥m t·∫Øt (ho·∫∑c 'exit' ƒë·ªÉ tho√°t):\n> ")
        if text.lower() == "exit":
            break
        print("\n‚ú® T√ìM T·∫ÆT:")
        print(generate_summary(model, text, tokenizer_x, tokenizer_y, MAX_TEXT_LEN, MAX_SUMM_LEN, DEVICE))
